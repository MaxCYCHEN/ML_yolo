{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079d614b-d1ce-4f24-b18d-5b61eff95307",
   "metadata": {},
   "source": [
    "# Prepare COCO data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea0f6e-5456-4e3e-830d-b1357a22065a",
   "metadata": {},
   "source": [
    "## check the downloaded dataset broken or not\n",
    "- This will delete the broken plots, and please redownload the broken plots again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e3de1d-305e-4c2d-98eb-8588ce24eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataset's plots broken or not\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def __load_images_from_folder(folder):  ### 使用yield，降低ram使用\n",
    "        count = 0\n",
    "        for filename in os.listdir(folder):\n",
    "            count += 1\n",
    "            yield filename\n",
    "def __check_dataset_nums(folder):\n",
    "    return len(os.listdir(folder))\n",
    "\n",
    "def is_valid(file):\n",
    "    valid = True\n",
    "    try:\n",
    "        Image.open(file).load()\n",
    "    except OSError:\n",
    "        valid = False\n",
    "    return valid\n",
    "\n",
    "def check_dataset_plot(dataset_loc):\n",
    "    total_size = __check_dataset_nums(dataset_loc)\n",
    "    with tqdm(total=total_size, desc='Check the broken plots') as pbar:\n",
    "        for img_filename in __load_images_from_folder(dataset_loc):\n",
    "            img_filename = os.path.join(dataset_loc, img_filename)\n",
    "            \n",
    "            if not is_valid(img_filename):\n",
    "                print(\"Broken and delete it: {}\".format(img_filename))\n",
    "                os.remove(img_filename)\n",
    "            pbar.update(1)    \n",
    "\n",
    "check_dataset_plot(r\"C:\\Users\\USER\\Desktop\\ML_tf2_object_detection_nu\\image_dataset\\coco-2017-person\\train\\data\")     \n",
    "#check_dataset_plot(r\"C:\\Users\\USER\\Desktop\\ML_tf2_object_detection_nu\\image_dataset\\coco-2017-person\\validation\\data\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcce610-e309-43e9-ba10-059931395287",
   "metadata": {},
   "source": [
    "## Convert json to txt (YOLO format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9488a90f-1098-4392-a781-5bb74b2d7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "191426f1-81ad-4e00-85d3-595d9a95ea74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] ['person']\n",
      "Create the yolo annotation files in: C:\\Users\\USER\\Desktop\\Yolo-Fastest-darknet\\training_demo\\output_yolo_person\\train\n",
      "Create the dataset path file: C:\\Users\\USER\\Desktop\\Yolo-Fastest-darknet\\training_demo\\output_yolo_person\\train.txt\n",
      "Create the classes file: C:\\Users\\USER\\Desktop\\Yolo-Fastest-darknet\\training_demo\\output_yolo_person\\coco.names\n",
      "[WinError 183] 當檔案已存在時，無法建立該檔案。: 'C:\\\\Users\\\\USER\\\\Desktop\\\\Yolo-Fastest-darknet\\\\training_demo\\\\output_yolo_person'\n",
      "skip create\n",
      "[1] ['person']\n",
      "Create the yolo annotation files in: C:\\Users\\USER\\Desktop\\Yolo-Fastest-darknet\\training_demo\\output_yolo_person\\val\n",
      "Create the dataset path file: C:\\Users\\USER\\Desktop\\Yolo-Fastest-darknet\\training_demo\\output_yolo_person\\val.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#json_file_name = 'C:/Users/CYCHEN38/image_detection/Yolo-FastestV2/image_dataset/coco-2017-50-2class/raw/instances_train2017.json'\n",
    "custom_dataset = r'C:\\Users\\USER\\Desktop\\ML_tf2_object_detection_nu\\image_dataset\\coco-2017-person'\n",
    "custom_dataset_output_yolo = r'C:\\Users\\USER\\Desktop\\Yolo-Fastest-darknet\\training_demo\\output_yolo_person'\n",
    "category_class = ['person']\n",
    "#category_class = ['all']\n",
    "#kind = 'validation'\n",
    "#kind = 'train'\n",
    "\n",
    "def process_kind_path(kind, custom_dataset_output_yolo):\n",
    "    if kind == 'validation':\n",
    "        output_ann_path = os.path.join(custom_dataset_output_yolo, 'val')\n",
    "        output_dataset_path = os.path.join(custom_dataset_output_yolo, ('val.txt')) # dataset path\n",
    "    else:    \n",
    "        output_ann_path = os.path.join(custom_dataset_output_yolo, kind)                # yolo format annotation folder\n",
    "        output_dataset_path = os.path.join(custom_dataset_output_yolo, (kind + '.txt')) # dataset path\n",
    "    return output_ann_path, output_dataset_path\n",
    "        \n",
    "def create_coco_yolo_data(custom_dataset, custom_dataset_output_yolo, category_class, kind): \n",
    "\n",
    "    custom_dataset_folder = os.path.join(custom_dataset, kind)\n",
    "    \n",
    "    #json_file_name = os.path.join(now_path, custom_dataset, 'raw', 'instances_val2017.json') \n",
    "    json_file_name = os.path.join(custom_dataset_folder, 'labels.json') #Json file of COCO\n",
    "    imgs_folder = os.path.join(custom_dataset_folder, 'data')           #images folder\n",
    "    \n",
    "    output_ann_path, output_dataset_path = process_kind_path(kind, custom_dataset_output_yolo)\n",
    "    json_tf = json2txt.json2txt(json_file_name, custom_dataset_output_yolo, output_ann_path, category_class)\n",
    "    json_tf.run_annotation(imgs_folder, output_ann_path)\n",
    "    json_tf.create_dataset_path_txt(output_ann_path, output_dataset_path)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(custom_dataset_output_yolo, 'coco.names')):\n",
    "        json_tf.create_classes(os.path.join(custom_dataset_output_yolo, 'coco.names')) # create the label file\n",
    "    \n",
    "create_coco_yolo_data(custom_dataset, custom_dataset_output_yolo, category_class, 'train')\n",
    "create_coco_yolo_data(custom_dataset, custom_dataset_output_yolo, category_class, 'validation')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67473a42-dd4e-49f7-8746-11a96cc5a696",
   "metadata": {},
   "source": [
    "# Get anchor bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb6026e-012a-4fed-84e8-7ca72105b775",
   "metadata": {},
   "source": [
    "# Build the training .data configuration file\n",
    "- Reference./data/coco.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed137921-64d2-44b7-b5b7-6cf0c8bfc98c",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19da25e-2f6d-484c-bf8d-c9c49b80c1e4",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
